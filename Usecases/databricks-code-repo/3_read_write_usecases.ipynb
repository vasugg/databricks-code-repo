{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain Read & Write Ops Assignment - Building Datalake & Lakehouse\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1425c429-48ba-40e0-b3b0-7358d4ba6e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign;\n",
    "create database if not exists telecom_catalog_assign.landing_zone;\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb1945d-61c2-4b84-b6de-d0a929b38ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write dbutils.fs code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a206ed7d-63ef-4946-afa7-3ad11f70e96c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = \"\"\" 101,Arun,31,Chennai,PREPAID \n",
    "102,Meera,45,Bangalore,POSTPAID \n",
    "103,Irfan,29,Hyderabad,PREPAID \n",
    "104,Raj,52,Mumbai,POSTPAID \n",
    "105,,27,Delhi,PREPAID \n",
    "106,Sneha,abc,Pune,PREPAID \"\"\"\n",
    "\n",
    "usage_tsv = \"\"\"customer_id\\tvoice_mins\\tdata_mb\\tsms_count \n",
    "101\\t320\\t1500\\t20 \n",
    "102\\t120\\t4000\\t5 \n",
    "103\\t540\\t600\\t52 \n",
    "104\\t45\\t200\\t2 \n",
    "105\\t0\\t0\\t0 \"\"\"\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp \n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54 \n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12 '''\n",
    "\n",
    "tower_logs_region2 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5002|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5003|104|TWR05|-75|2025-01-10 11:01:12'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179ff3bf-457c-4e04-a5d1-6ae9aaf8792e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", customer_csv, True)\n",
    "df2=dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\", usage_tsv, True)  \n",
    "df3=dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\", tower_logs_region1, True)\n",
    "df4=dbutils.fs.put(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv\", tower_logs_region2, True)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Spark Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe690bdd-a64c-40fa-b59b-04eb4967c25a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#recursiveFileLookup=True, reads files from the subfolders too\n",
    "#pathGlobFilter=\"tower_logs_*\", reads files with the pattern starts with tower_logs_, if file name unknown, we can use *.csv\n",
    "df_multiple_path_files = (spark.read.option\n",
    "                          ('header', True)\n",
    "                          .option('inferSchema', True)\n",
    "                          .option('delimiter', '|')\n",
    "                          .option('recursiveFileLookup', True)\n",
    "                          .option('pathGlobFilter', 'tower*')\n",
    "                          .text('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/*')\n",
    ")\n",
    "                         \n",
    "display(df_multiple_path_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18e2068-4354-4afe-a178-d65b810dcd91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = (spark.read.format(\"csv\")\n",
    "       .options(header=\"false\", inferSchema=\"true\")\n",
    "       .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "       )\n",
    "df1.printSchema\n",
    "display(df1)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5b88102-7a56-4c81-8afa-441ef344d13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. When using header=True: The first row of data is treated as the column names.\n",
    "1. When using header=False: Columns are automatically assigned default names like c0, c1, c2, etc.\n",
    "1. Using toDF with header=False: User-specified column names will replace the default column names.\n",
    "1. Using toDF with header=True: User-specified column names will replace the default ones, but the first row of data will be discarded (leading to data loss, which is not recommended).\n",
    "1. inferSchema=True: The column data types will be inferred based on the data itself.\n",
    "1. inferSchema=False: The default data type for all columns will be String.\n",
    "\n",
    "1. The Age column is considered as String because Sneha’s age is mentioned as \"abc\".\n",
    "1. If all values in the Age column were numeric, the data type would be inferred as Integer.\n",
    "1. A null value is present in the custname column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d18b3736-4262-4683-bf0a-655df8ecfea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Apply column names using string using toDF function for customer data\n",
    "df_customer = (\n",
    "            spark.read.options(inferSchema=\"true\")\n",
    "            .format(\"csv\")\n",
    "            .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "            .toDF(\"customer_id\",\"name\",\"age\",\"city\",\"plan\")\n",
    "      )\n",
    "display(df_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b90612f-ef5b-456c-b8a9-484737d94802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Apply column names and datatype using the schema function for usage data\n",
    "str_struct=\"customer_id integer, voice_mins integer, data_mb integer, sms_count string\"\n",
    "\n",
    "df_usage_use_schema = (\n",
    "               spark.read.schema(str_struct)\n",
    "               .options(header=True, sep=\"\\t\")\n",
    "               .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "             )\n",
    "display(df_usage_use_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e326500-b5d6-4f87-ba71-456de6b0f337",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType, TimestampType\n",
    "\n",
    "cust_schema=StructType(\n",
    "    [\n",
    "        StructField(\"event_id\",IntegerType(),True),\n",
    "        StructField(\"customer_id\",IntegerType(),True),\n",
    "        StructField(\"tower_id\",StringType(),True),\n",
    "        StructField(\"signal_strength\",IntegerType(),True),\n",
    "        StructField(\"timestamp\",TimestampType(),True)\n",
    "    ]\n",
    ")\n",
    "df_logs_cust_schema = (\n",
    "                    spark.read.schema(cust_schema)\n",
    "                    .options(header=True,sep=\"|\")\n",
    "                    .format(\"csv\")\n",
    "                    .load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\")\n",
    "                 )\n",
    "#display(df_logs_cust_schema)\n",
    "\n",
    "\n",
    "df_logs_cust_schema.printSchema()\n",
    "df_logs_cust_schema.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e34c3bc-962d-438d-a1b6-ac27d2da6608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9a8974e-adef-4a69-bd63-663670dd55f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Write customer data into CSV format using overwrite mode\n",
    "\n",
    "# Create a sample DataFrame (replace with your actual data)\n",
    "cust_schema=\"id int,name string,age string,city string,plan string\"\n",
    "customer_data_df = spark.read.schema(cust_schema).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "output_path = \"dbfs:///Volumes/telecom_catalog_assign/transform_zone/csv/customer/customer.csv\"\n",
    "\n",
    "customer_df = customer_data_df.write.mode(\"overwrite\").format(\"csv\").save(output_path)\n",
    "customer_data_df.write.mode(\"overwrite\").format(\"csv\").save(output_path,compression='gzip')\n",
    "\n",
    "#display(customer_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2721503c-6e1a-4ed6-b3e7-860dfb994e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Write usage data into CSV format using append mode\n",
    "\n",
    "new_usage_data_df = spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",sep='\\t',header=True)\n",
    "\n",
    "output_path = \"dbfs:///Volumes/telecom_catalog_assign/transform_zone/csv/usage/usage.csv\"\n",
    "\n",
    "# #write the usage dataframe to csv in append mode\n",
    "new_usage_data_df = new_usage_data_df.write \\\n",
    "                    .format(\"csv\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .save(output_path)\n",
    "\n",
    "print(f\"Data appended to {output_path} successfully in append mode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1540847b-a67e-4033-8c87-fc9d4e232c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "\n",
    "cust_schema=StructType(\n",
    "    [\n",
    "        StructField(\"event_id\",IntegerType(),True),\n",
    "        StructField(\"customer_id\",IntegerType(),True),\n",
    "        StructField(\"tower_id\",StringType(),True),\n",
    "        StructField(\"signal_strength\",StringType(),True),\n",
    "        StructField(\"timestamp\",StringType(),True)\n",
    "    ]\n",
    ")\n",
    "df_tower_logs = spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",sep='|',header=True)\n",
    "\n",
    "# Define the output path\n",
    "output_path = \"dbfs:///Volumes/telecom_catalog_assign/transform_zone/csv/tower/region1/tower_logs_region1.csv\" \n",
    "\n",
    "# Write the DataFrame to CSV format with a pipe separator\n",
    "df_tower_logs = df_tower_logs.write.csv(output_path,sep='|',header=True)\n",
    "\n",
    "print(f\"Tower data successfully written to {output_path} with pipe separator.\")\n",
    "\n",
    "df_tower_logs_transform = spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/csv/tower/region1/tower_logs_region1.csv\",sep='|',header=True)\n",
    "df_tower_logs_transform.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34158cf6-dd7f-40d6-9969-ed76710540a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3ee5ef-5561-4070-8b27-4b229b6f5e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Write customer data into JSON format using overwrite mode \n",
    "\n",
    "#Read customer data:\n",
    "schema1=\"id int,name string,age string,city string,plan string\"\n",
    "df1=spark.read.schema(schema1).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "#write customer data\n",
    "df1.write.mode(\"overwrite\").format(\"json\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/json/customer/customer.json\")\n",
    "df1.write.mode(\"overwrite\").format(\"json\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/json/customer_gzip/customer.json\",compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b22d3e2-24b6-4b82-a003-01af69e24597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Write usage data into JSON format using append mode and snappy compression format\n",
    "\n",
    "#Read usage data\n",
    "df2=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",sep='\\t',header=True)\n",
    "#write usage data\n",
    "df2.write.mode(\"append\").format(\"json\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/json/usage/usage.json\",compression=\"snappy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ef83a7-2e6d-478c-bbb3-9fdc2955c23c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "\n",
    "#read tower data\n",
    "df3=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",sep='|',header=True)\n",
    "#write tower data\n",
    "df3.write.mode(\"ignore\").json(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/json/tower/region1/tower_logs_region1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26f2ba69-3cde-4ec6-8945-e4ef9f7bb109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51b3bd1-b452-4c57-9cd6-3553665552cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "#Read customer data:\n",
    "schema1=\"id int,name string,age string,city string,plan string\"\n",
    "df1=spark.read.schema(schema1).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "#write customer data\n",
    "df1.write.mode(\"overwrite\").format(\"parquet\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/parquet/customer/customer.parquet\")\n",
    "\n",
    "#2. Write usage data into Parquet format using error mode\n",
    "#read usage data\n",
    "df2=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\",header=True)\n",
    "\n",
    "#write usage data\n",
    "df2.write.mode(\"error\").format(\"parquet\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/parquet/usage/usage.parquet\")\n",
    "\n",
    "#3. Write tower data into Parquet format with gzip compression option\n",
    "#read tower data\n",
    "df3=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",sep='|',header=True)\n",
    "#write tower data\n",
    "df3.write.mode(\"overwrite\").parquet(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/parquet/tower/region1/tower_logs_region1.parquet\",compression=\"gzip\")\n",
    "\n",
    "\n",
    "#read parquet tower data from transform zone path\n",
    "df4=spark.read.parquet(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/parquet/usage/usage.parquet\")\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b41c794f-5cfc-4aeb-a599-e6d4a47a0f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##9. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972898cc-76a1-4726-a4d8-774df2706290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Write customer data into ORC format using overwrite mode\n",
    "schema1=\"id int,name string,age string,city string,plan string\"\n",
    "df1=spark.read.schema(schema1).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "#write customer data\n",
    "df1.write.mode(\"overwrite\").format(\"orc\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/orc/customer/customer.orc\")\n",
    "df1.write.mode(\"overwrite\").format(\"orc\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/orc/customer_zstd/customer.orc\",compression=\"zstd\")\n",
    "\n",
    "#2. Write usage data into ORC format using append mode\n",
    "#read usage data\n",
    "df2=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",sep='\\t',header=True)\n",
    "#write usage data\n",
    "df2.write.mode(\"append\").format(\"orc\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/orc/usage/usage.orc\")\n",
    "\n",
    "#3. Write tower data into ORC format and see the output file structure\n",
    "#read tower logs data\n",
    "df3=spark.read.csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",sep='|',header=True)\n",
    "#write tower data\n",
    "df3.write.mode(\"overwrite\").orc(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/orc/tower/region1/tower_logs_region1.orc\",compression=\"snappy\")\n",
    "\n",
    "#4. Read the usage data in a dataframe and show only 5 rows.\n",
    "df4=spark.read.orc(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/orc/usage/usage.orc\")\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35761315-0b0f-46ff-9c3d-c0405bce7b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1a21e6-60f3-4ee9-bf51-c0c6e4a0171d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#1. Write customer data into Delta format using overwrite mode\n",
    "#read cust data\n",
    "schema1=\"id int,name string,age string,city string,plan string\"\n",
    "df1=spark.read.schema(schema1).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "#write customer data\n",
    "df1.write.mode(\"overwrite\").format(\"delta\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/delta/customer/customer.delta\")\n",
    "\n",
    "#2. Write usage data into Delta format using append mode\n",
    "#read usage data\n",
    "schema2=\"cutomer_id int,voice_mins int,data_mb int,sms_count int\"\n",
    "df2=spark.read.schema(schema2).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",sep='\\t',header=True)\n",
    "\n",
    "#write usage data\n",
    "df2.write.mode(\"overwrite\").format(\"delta\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/delta/usage/usage.delta\")\n",
    "\n",
    "#3. Write tower data into Delta format and see the output file structure\n",
    "#read tower logs data\n",
    "schema3=\"event_id int,customer_id int,tower_id string,signal_strength string, timestamp timestamp\"\n",
    "df3=spark.read.schema(schema3).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",sep='|',header=True)\n",
    "\n",
    "#write tower data\n",
    "df3.write.mode(\"overwrite\").format(\"delta\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/delta/tower/region1/tower_logs_region1.delta\")\n",
    "\n",
    "#4. Read the usage data in a dataframe and show only 5 rows.\n",
    "df4=spark.read.format(\"delta\").load(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/delta/usage/usage.delta\")\n",
    "df4.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6dd0890-02bd-4acd-b837-daceb256c706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db91c2bc-12ac-426f-b5dd-027138e396d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#1. Write customer data using saveAsTable() as a managed table\n",
    "#read cust data\n",
    "schema1=\"id int,name string,age string,city string,plan string\"\n",
    "df1=spark.read.schema(schema1).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "#write customer data\n",
    "df1.write.mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.transform_zone.customer\")\n",
    "\n",
    "#2. Write customer data using saveAsTable() as a managed table\n",
    "#read usage data\n",
    "schema2=\"cutomer_id int,voice_mins int,data_mb int,sms_count int\"\n",
    "df2=spark.read.schema(schema2).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",sep='\\t',header=True,inferSchema=True)\n",
    "#write usage data\n",
    "df2.write.mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.transform_zone.usage\")\n",
    "\n",
    "#3. Drop the managed table and verify data removal\n",
    "#read_towe_data\n",
    "schema3=\"event_id int,customer_id int,tower_id string,signal_strength string, timestamp timestamp\"\n",
    "df3=spark.read.schema(schema3).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",sep='|',header=True,inferSchema=True)\n",
    "#write tower data\n",
    "df3.write.mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.transform_zone.tower_logs_region1\")\n",
    "\n",
    "#4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "df4=spark.sql(\"select sum(voice_mins) as total_voice_mins,sum(data_mb) as total_data_mb, sum(sms_count) as total_sms_count from telecom_catalog_assign.transform_zone.usage\")\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aac447b-690b-4562-99dd-0ce096e9ad55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c774c610-95c0-49f1-b9e0-5b97b2a81a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1=spark.sql(\"select sum(voice_mins) as total_voice_mins,sum(data_mb) as total_data_mb, sum(sms_count) as total_sms_count from telecom_catalog_assign.transform_zone.usage\") \n",
    "df1.write.mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.transform_zone.usage_summary\")\n",
    "df1.write.insertInto(\"telecom_catalog_assign.transform_zone.usage_summary\",overwrite=True)\n",
    "\n",
    "#or\n",
    "\n",
    "df1.createOrReplaceTempView(\"usage_summary_temp\")\n",
    "spark.sql(\"insert overwrite table telecom_catalog_assign.transform_zone.usage_summary select * from usage_summary_temp\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3c4bce3-4bd3-4db6-a074-02bb24c5f91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b81a8dae-5cc7-4260-aba1-4bc945065151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#1. Write customer data into XML format using rowTag as cust\n",
    "#read cust data\n",
    "schema1=\"id int,name string,age string,city string,plan string\"\n",
    "df1=spark.read.schema(schema1).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "#write customer data\n",
    "df1.write.mode(\"overwrite\").option(\"rowTag\",\"customer\").format(\"xml\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/xml/customer/customer.xml\")\n",
    "df1.write.mode(\"overwrite\").option(\"rowTag\",\"customer\").format(\"xml\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/xml/customer_gzip/customer.xml\",compression=\"gzip\")\n",
    "\n",
    "#2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "#read usage data\n",
    "schema2=\"cutomer_id int,voice_mins int,data_mb int,sms_count int\"\n",
    "df2=spark.read.schema(schema2).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",sep='\\t',header=True,inferSchema=True)\n",
    "\n",
    "#write usage data\n",
    "df2.write.mode(\"overwrite\").option(\"rowTag\",\"usage_metrics\").format(\"xml\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/xml/usage/usage1.xml\")\n",
    "\n",
    "#3. Write tower log data into XML format using overwrite mode with the rowTag as tower_region1_metrics\n",
    "#read_tower_data\n",
    "schema3=\"event_id int,customer_id int,tower_id string,signal_strength string, timestamp timestamp\"\n",
    "df3=spark.read.schema(schema3).csv(\"dbfs:///Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",sep='|',header=True,inferSchema=True)\n",
    "\n",
    "#write tower data\n",
    "df3.write.mode(\"overwrite\").option(\"rowTag\",\"tower_region1_metrics\").format(\"xml\").save(\"dbfs:///Volumes/telecom_catalog_assign/transform_zone/xml/tower/region1/tower_logs_region1.xml\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e2fe69-9352-4ec9-bf70-15d760c89aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "280e93cd-396a-46e2-943f-8004a8971831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d6e39ec-752d-4183-9656-2b6d7938922d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##15. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cd9a85f-91c9-4e16-8a02-20d94c32090f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "When to use/benifits csv ->\n",
    "\n",
    "  storing raw data\n",
    "  Occupies more space\n",
    "  doesnt work well with gzip unlike xml or json\n",
    "When to use/benifits json ->\n",
    "\n",
    "  storing api logs\n",
    "  gives better compression with gzip\n",
    "  stroing unstructred data\n",
    "When to use/benifit orc ->\n",
    "\n",
    "  columar file format, \n",
    "  works well with zstd(better storage) also snappy(speed) to some extent\n",
    "  hive tables\n",
    "  default compression is snappy\n",
    "When to use/benifit parquet\n",
    "\n",
    "    default compression is snappy\n",
    "    columar file type\n",
    "    faster read and write\n",
    "    comparitively greater storage\n",
    "When to use/benifit delta\n",
    "\n",
    "    default parquet file with snappy compression (databricks default)\n",
    "    columar file type\n",
    "    faster read and write\n",
    "    comparitively greater storage\n",
    "    acid trasanctions\n",
    "    time travel with help of delta logs\n",
    "When to use/benifit xml\n",
    "\n",
    "xml with gzip - maximum compression\n",
    "xmls used in legacy systems\n",
    "When to use/benifit delta tables\n",
    "\n",
    "tables with updates\n",
    "acid tables\n",
    "requires timetravel"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8993999461363826,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3_read_write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
